{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78cada71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/apanda38/cs8803/final/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from more_itertools import chunked\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36cabb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [03:16<00:00,  6.54s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "model_name_short = \"Llama-3.1-70B-Instruct\"\t\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-70B-Instruct\", torch_dtype=\"bfloat16\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-70B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90e56703",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = \"caa_sv\"\n",
    "\n",
    "num_layers = len(model.model.layers)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4804fba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = \"persona\"\n",
    "# behaviors = [\n",
    "#     \"openness\",\n",
    "#     \"conscientiousness\",\n",
    "#     \"extraversion\",\n",
    "#     \"agreeableness\",\n",
    "#     \"neuroticism\",\n",
    "#     \"politically-liberal\"\n",
    "# ]\n",
    "data_dir = \"advanced-ai-risk/lm_generated_evals\"\n",
    "behaviors = [\n",
    "    \"power-seeking-inclination\",\n",
    "    \"self-awareness-general-ai\",\n",
    "    \"corrigible-neutral-HHH\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5fe454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_special_tokens(questions, pos_answers, neg_answers):\n",
    "    pos_prompts = [f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{q}<|eot_id|><|start_header_id|>assistant<|end_header_id|>{a}\" for q, a in zip(questions, pos_answers)]\n",
    "    neg_prompts = [f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{q}<|eot_id|><|start_header_id|>assistant<|end_header_id|>{a}\" for q, a in zip(questions, neg_answers)]\n",
    "    \n",
    "    return pos_prompts, neg_prompts\n",
    "\n",
    "### HELPER FUNCTIONS\n",
    "\n",
    "def compute_sv(pos_prompts, neg_prompts, tokenizer, model):\n",
    "    bs = 5\n",
    "    steering_vectors = None\n",
    "    for pos, neg in zip(chunked(pos_prompts, bs), chunked(neg_prompts, bs)):\n",
    "        contrastive_statements = pos + neg\n",
    "        input_ids_contrastive = tokenizer(contrastive_statements, padding=True, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
    "        hidden_states = []\n",
    "        with torch.inference_mode():\n",
    "            hidden_states = model(**input_ids_contrastive, output_hidden_states=True)[\"hidden_states\"] #L, B, S, H\n",
    "            num_pos = len(pos)\n",
    "            layer_vec = []\n",
    "\n",
    "            for layer in range(len(hidden_states)):\n",
    "                final_token_reps = hidden_states[layer][:, -1, :] #B, H\n",
    "                avg_pos = final_token_reps[:num_pos].mean(dim=0)\n",
    "                avg_neg = final_token_reps[num_pos:].mean(dim=0)\n",
    "                layer_vec.append((avg_pos - avg_neg).to(\"cuda:0\"))\n",
    "\n",
    "            if steering_vectors == None:\n",
    "                steering_vectors = torch.stack(layer_vec)[1:] / (len(pos_prompts) / bs)\n",
    "            else:\n",
    "                steering_vectors += torch.stack(layer_vec)[1:] / (len(pos_prompts) / bs)\n",
    "\n",
    "            del hidden_states, input_ids_contrastive\n",
    "            gc.collect()\n",
    "            torch.cuda.synchronize()\n",
    "            torch.cuda.empty_cache()\n",
    "    return steering_vectors\n",
    "\n",
    "def write_steering_vectors_to_json(model_name, target_name, split, steering_vectors, output_dir=OUT_DIR):\n",
    "    filename = f\"{model_name}-{target_name}-{split}.json\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    steering_list = steering_vectors.cpu().tolist()\n",
    "\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(steering_list, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed0f9a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "for behavior in behaviors:\n",
    "    if behavior == \"openness\":\n",
    "        ds = load_dataset(\"json\", data_files=f\"{behavior}.jsonl\", split=\"train[:80%]\")\n",
    "    else:\n",
    "        ds = load_dataset(\"Anthropic/model-written-evals\", data_dir=data_dir, data_files=f\"{behavior}.jsonl\", split=\"train[:80%]\")\n",
    "    train_len = len(ds)\n",
    "    questions = ds[0:][\"question\"]\n",
    "    pos_answers = ds[0:][\"answer_matching_behavior\"]\n",
    "    neg_answers = ds[0:][\"answer_not_matching_behavior\"]\n",
    "\n",
    "    pos_prompts, neg_prompts = add_special_tokens(questions, pos_answers, neg_answers)\n",
    "\n",
    "    sv = compute_sv(pos_prompts, neg_prompts, tokenizer, model)\n",
    "\n",
    "    write_steering_vectors_to_json(model_name_short, behavior, f\"{train_len}\", sv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
